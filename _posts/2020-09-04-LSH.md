---
title: "Locality-Sensitive Hashing Scheme Based on Dynamic Collision Counting"
date: 2020-09-04
tags: [PySpark, Similarity Search, Big Data]
excerpt: "Implementation of LSH scheme by counting close collisions between similar items using PySpark RDD's"
mathjax: "true"
---

### Similarity Search in High Dimensional Space

Similarity search is the problem of finding all o in the Data set, where o is similar to a query q.

In low dimensional space there exist two common methods of similarity search:
* Range Search: Find all points such that the distance between o and q is less than some value.
  $$dist(o,q) \leq \tau$$
* Nearest Neighbour Search: Find the closest k points.
  $$dist(o*,q) \leq dist(o,q), \forall o \in Data$$
The distance/similarity function varies, eg Euclidean, Jaccard, inner product.

However in high dimensional space traditional methods fall apart. This is due to the curse of dimentionality, in which various phenomena occur that do not occur in low dimensional space. This illustrated in the example below. 

![](/images/lsh1.png)

However we can approximate Nearest Neighbor Search in high dimensional space, this can be achieved by using hashing!

### Locality Sensitive Hashing (LSH)

LSH works by hashing each data point into a 'bucket'. We can then hash our query. Now we can obtain all the data that has the same 'bucket', this is achieved in O(1) time for each data point, so O(c) for c 'candidates'. We can then find the nearest of these candidates. 
It is important that our hash functions are error tollerant, ie, similar data is returned as a candidate with high probability and dissimilar data returned with low probability, thus there are likely to be many false positive candidates returned. 
Formally our chosen LSH function h should satisfy:

$$Pr[h(o) = h(q)] \geq p_1, \text{if } dist(o,q) \leq r_1$$
$$Pr[h(o) = h(q)] \leq p_2, \text{if } dist(o,q) > r_2$$

Where o is a candidate, q is a query and our probability <img src="https://latex.codecogs.com/gif.latex?p_1" title="p_1" /> is high, <img src="https://latex.codecogs.com/gif.latex?p_2" title="p_2" /> is low and r is some threshold for what we define as similar.

LSH Functions and their metric for similarity include:
* MinHash - Jaccard similarity 
* SimHash - Angular distance 
* P-Stable - Euclidean Distance

Additionally, for a single hash function let:

$$Pr[h(o) = h(q)] = p_{oq}$$

We can combine k hashes together, thus the probability that o and q match on all the hashes is:

$$p_{oq}^k$$

We can also combine L hashes together, thus the probability that o and q match on at least one of L hashes is:

$$1-(1-p_{oq})^L$$

Thus if we have L groups of k hashes the probability that o is a nearest neighbour candidate of q is:

$$1-(1-p_{oq}^k)^L$$

**Example**:
For the MinHash scheme we have:

$$p_{oq} = Jaccard(o,q)$$

With k and L equal to 5 we have:

![](/images/lsh2.png)

Thus if we will retrieve 98.8 percent of the data with Jaccard greater than 0.9.

### Collision Counting LSH (C2LSH)

[This](http://www.cs.ust.hk/~wilfred/paper/sigmod12.pdf) paper outlines a method in which we can always find at least a certain number of most similar points

We have the problem, which of <img src="https://latex.codecogs.com/gif.latex?o_1" title="o_1" /> and <img src="https://latex.codecogs.com/gif.latex?o_2" title="o_2" />
is more similar to <img src="https://latex.codecogs.com/gif.latex?q" title="q" />:

![](/images/lsh3.png)

We set a paramter 
$$\alpha m$$ 
that is the number of 'collisions' or equal hash values that we require to consider an item similar. Eg, 
$$o_1$$
 above has 9 collisions with 
 $$q$$
 . 
We also set a parameter 
$$\beta m$$
 that is the minimum number of candidates that we want to return, or the minimum number of similar items. 

However if we do not have enough candidates we virtual rehash (increase an offset so a collision is not equal, but is within 1, 2, 3...):

![](/images/lsh4.png)

We now implement C2LSH using PySpark RDD's with dummy data for demonstration. In order to decrease runtime the algorithm is altered to perform a binary search on the optimal offset.

```python
import math

def c2lsh(data_hashes, query_hashes, alpha_m, beta_n):

    #Map to (key, |data_hashes[1] - query_hashes|)
    absdiff_rdd = data_hashes.map(lambda x:(x[0], [abs(a - b) for a, b in zip(x[1], query_hashes)]))
    absdiff_rdd.cache()
    
    #binary serach for offset between 0 and max(|data_hashes[1] - query_hashes|)
    min_offset = 0
    max_offset = int(absdiff_rdd.map(lambda x: max(x[1])).max()) 

    return caclCand(absdiff_rdd, min_offset, max_offset, alpha_m, beta_n)

def caclCand(absdiff_rdd, loffset, roffset, alpha_m, beta_n):
    #new offset   
    new_offset = math.floor((loffset + roffset) / 2)

    #Candidates have number of collisions >= am
    cand_rdd = absdiff_rdd.filter(lambda x: sum(i <= new_offset for i in x[1]) >= alpha_m).keys()

    #Length of candidate set
    count = cand_rdd.count()
    
    #If we reach the end of our binary search
    if (loffset == roffset - 1):
        if (count >= beta_n):
            return cand_rdd
        else:
            return absdiff_rdd.filter(lambda x: sum(i <= roffset for i in x[1]) >= alpha_m).keys()
    
    #Perform binary search
    if (count > beta_n):
        roffset = new_offset
        return caclCand(absdiff_rdd, loffset, roffset, alpha_m, beta_n)
    else:
        loffset = new_offset
        return caclCand(absdiff_rdd, loffset, roffset, alpha_m, beta_n)
```


```python
from pyspark import SparkContext, SparkConf
from time import time
import random

def createSC():
    conf = SparkConf()
    conf.setMaster("local[*]")
    conf.setAppName("C2LSH")
    sc = SparkContext(conf = conf)
    return sc

def generate(dimension, count, seed, buckets):
    random.seed(seed)

    data = [[random.randint(0, buckets) for _ in range(dimension)]
            for i in range(count)
            ]

    query = [random.randint(0, buckets) for _ in range(dimension)]
    
    return data, query

#Generate 100,000 dummy data points representing data that has been hashed into 1000 buckets. Also generate a random query.
data, query_hashes = generate(10, 100000, 0, 1000)

print("First hashed data point: ", data[0])
print("Hashed query point: ", query_hashes)
```

    First hashed data point:  [864, 394, 776, 911, 430, 41, 265, 988, 523, 497]
    Hashed query point:  [268, 844, 940, 650, 700, 610, 222, 508, 925, 305]
    


```python
#Set Parameters
alpha_m = 4 #Minimum collisions for object to be considered similar
beta_n = 10 #Minumum number of returned candidates

#Create our Spark context
sc = createSC()
#Data into RDD
length = len(data) - 1
data_hashes = sc.parallelize([(length - index, x) for index, x in enumerate(data)])

#Run C2LSH
start_time = time()
res = c2lsh(data_hashes, query_hashes, alpha_m, beta_n).collect()
end_time = time()
sc.stop()

print('running time:', end_time - start_time)
print('Number of candidate: ', len(res))
print('Key of candidate: ', set(res))
```

    running time: 27.700690746307373
    Number of candidate:  12
    Key of candidate:  {36515, 55561, 94766, 32816, 80625, 47665, 32947, 40758, 21402, 80859, 69564, 59390}
    


### Applications

Almost every object can be represented by a high dimensional vector, eg words documents, images and audio. 
It thus similarity search in high dimensional space becomes an important problem in this context.
For example facial recognition systems and search engines.

As a simple application we can try and find similar images of hand drawn digits. 