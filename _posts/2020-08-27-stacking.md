---
title: "Ensemble Learning: Model Stacking"
date: 2020-08-27
tags: [machine learning, big data, pyspark]
header:
  image: "/images/michael-dziedzic-nbW-kaz2BlE-unsplash.jpg"
excerpt: "Text Classification using Model Stacking on Big Data"
mathjax: "true"
---

### Data

The dataset consists of sentences from customer reviews of different restaurants. There are 2241, 800, 800 customer reviews in the train, development, and test datasets, respectively. Our task is to identify the category of each customer review using the review text and the trained model.
The categories include:<br/>
* FOOD: reviews that involve comments on the food.
e.g. “All the appetizers and salads were fabulous , the steak was mouth watering and the pasta was delicious”
* PAS: reviews that only involve comments on price, ambience, or service.
e.g. “Now it 's so crowded and loud you ca n't even talk to the person next to you”
* MISC: reviews that do not belong to the above categories including sentences that are general recommendations reviews describing the reviewer’s personal experience or context, but that do not usually provide information on the restaurant quality
e.g. “Your friends will thank you for introducing them to this gem!”
e.g. “I knew upon visiting NYC that I wanted to try an original deli”

Firstly we import all the packages needed for data cleaning and model buidling. We utilise the Spark Eco-System that enables efficent manipulation of large data sets through the use of computing clustering and data parallelism.

```python
from pyspark.sql import *
from pyspark import SparkConf

from pyspark.sql import DataFrame
from pyspark.sql.functions import rand, udf
from pyspark.sql.types import IntegerType, DoubleType

from pyspark.ml import Pipeline, Transformer
from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler
from pyspark.ml.classification import LogisticRegression, LinearSVC, NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import Tokenizer, CountVectorizer, StringIndexer
```
We can now import our data.
```python
# Create a Spark Session
conf = SparkConf().setMaster("local[*]").setAppName("lab3")
spark = SparkSession.builder.config(conf=conf).getOrCreate()

# Load data
train_data = spark.read.load("proj2train.csv", format="csv", sep="\t", inferSchema="true", header="true")
test_data = spark.read.load("proj2test.csv", format="csv", sep="\t", inferSchema="true", header="true")

train_data.show()
```

    +---+--------+--------------------+
    | id|category|            descript|
    +---+--------+--------------------+
    |  0|    MISC|I've been there t...|
    |  1|    FOOD|Stay away from th...|
    |  2|    FOOD|Wow over 100 beer...|
    |  3|    MISC|Having been a lon...|
    |  4|    MISC|This is a consist...|
    |  5|    FOOD|I ate here a week...|
    |  6|    MISC|First of all Dal ...|
    |  7|    FOOD|Great food at REA...|
    |  8|    FOOD|While there are p...|
    |  9|    MISC|My first encounte...|
    | 10|    FOOD|one of the best C...|
    | 11|     PAS|But the pizza is ...|
    | 12|    MISC|Turned out there ...|
    | 13|    FOOD|My entree of hot ...|
    | 14|    MISC|I will have to sa...|
    | 15|     PAS|The seats are unc...|
    | 16|    MISC|Please save yours...|
    | 17|    FOOD|The food is consi...|
    | 18|    MISC|--Eat Club is a r...|
    | 19|     PAS|Good atmosphere, ...|
    +---+--------+--------------------+
    only showing top 20 rows

### Data Preparation

We now need to prepare our data. In base_features_gen_pipeline we transform the descript of each item into a bag-of-words representation as well as binarizing the categories. Next, each item is assigned a group, to later be used in k-fold cross validation and one hot encoding is then performed on the labels through gen_binary_labels.

```python
def base_features_gen_pipeline(input_descript_col="descript", input_category_col="category", output_feature_col="features", output_label_col="label"):
    
    # white space expression tokenizer
    word_tokenizer = Tokenizer(inputCol="descript", outputCol="words")

    # bag of words count
    count_vectors = CountVectorizer(inputCol="words", outputCol="features")

    # label indexer
    label_maker = StringIndexer(inputCol = "category", outputCol = "label")
    
    # to dataframe output
    class Selector(Transformer):
        def __init__(self, outputCols=['id', 'features', 'label']):
            self.outputCols=outputCols

        def _transform(self, df: DataFrame) -> DataFrame:
            return df.select(*self.outputCols)

    selector = Selector(outputCols = ['id', 'features', 'label'])
    
    # build the pipeline
    pipeline = Pipeline(stages=[word_tokenizer, count_vectors, label_maker, selector])
    
    return pipeline

import random
rseed = 1024
random.seed(rseed)
def gen_binary_labels(df):
    df = df.withColumn('label_0', (df['label'] == 0).cast(DoubleType()))
    df = df.withColumn('label_1', (df['label'] == 1).cast(DoubleType()))
    df = df.withColumn('label_2', (df['label'] == 2).cast(DoubleType()))
    return df

# build the pipeline 
base_features_pipeline = base_features_gen_pipeline()
# Fit the pipeline using train_data
base_features_pipeline_model = base_features_pipeline.fit(train_data)
# Transform the train_data using fitted pipeline
training_set = base_features_pipeline_model.transform(train_data)
# assign random groups and binarize the labels
training_set = training_set.withColumn('group', (rand(rseed)*5).cast(IntegerType()))
training_set = gen_binary_labels(training_set)
training_set.show()
```

    +---+--------------------+-----+-----+-------+-------+-------+
    | id|            features|label|group|label_0|label_1|label_2|
    +---+--------------------+-----+-----+-------+-------+-------+
    |  0|(5421,[1,18,31,39...|  1.0|    4|    0.0|    1.0|    0.0|
    |  1|(5421,[0,1,15,20,...|  0.0|    4|    1.0|    0.0|    0.0|
    |  2|(5421,[3,109,556,...|  0.0|    4|    1.0|    0.0|    0.0|
    |  3|(5421,[1,2,3,5,6,...|  1.0|    0|    0.0|    1.0|    0.0|
    |  4|(5421,[2,3,4,8,11...|  1.0|    2|    0.0|    1.0|    0.0|
    |  5|(5421,[1,2,5,25,4...|  0.0|    0|    1.0|    0.0|    0.0|
    |  6|(5421,[7,40,142,1...|  1.0|    4|    0.0|    1.0|    0.0|
    |  7|(5421,[8,13,19,25...|  0.0|    4|    1.0|    0.0|    0.0|
    |  8|(5421,[2,3,7,8,21...|  0.0|    4|    1.0|    0.0|    0.0|
    |  9|(5421,[2,16,22,49...|  1.0|    4|    0.0|    1.0|    0.0|
    | 10|(5421,[0,7,47,49,...|  0.0|    1|    1.0|    0.0|    0.0|
    | 11|(5421,[0,3,4,14,7...|  2.0|    1|    0.0|    0.0|    1.0|
    | 12|(5421,[1,6,29,41,...|  1.0|    2|    0.0|    1.0|    0.0|
    | 13|(5421,[1,2,6,7,16...|  0.0|    3|    1.0|    0.0|    0.0|
    | 14|(5421,[3,4,5,11,1...|  1.0|    0|    0.0|    1.0|    0.0|
    | 15|(5421,[0,15,20,21...|  2.0|    3|    0.0|    0.0|    1.0|
    | 16|(5421,[0,525,853,...|  1.0|    3|    0.0|    1.0|    0.0|
    | 17|(5421,[0,1,4,8,13...|  0.0|    1|    1.0|    0.0|    0.0|
    | 18|(5421,[2,4,7,221,...|  1.0|    3|    0.0|    1.0|    0.0|
    | 19|(5421,[0,4,7,20,2...|  2.0|    2|    0.0|    0.0|    1.0|
    +---+--------------------+-----+-----+-------+-------+-------+
    only showing top 20 rows

### Base Model
